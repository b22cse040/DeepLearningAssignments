{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZJ6oPrK8Yf8",
        "outputId": "81d42905-4d0c-4ee6-caaa-b46980021569"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: skorch in /usr/local/lib/python3.11/dist-packages (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from skorch) (1.26.4)\n",
            "Requirement already satisfied: scikit-learn>=0.22.0 in /usr/local/lib/python3.11/dist-packages (from skorch) (1.6.1)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from skorch) (1.13.1)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.11/dist-packages (from skorch) (0.9.0)\n",
            "Requirement already satisfied: tqdm>=4.14.0 in /usr/local/lib/python3.11/dist-packages (from skorch) (4.67.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.22.0->skorch) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.22.0->skorch) (3.5.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install skorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_YG1EPrK4uFu"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from skorch import NeuralNetClassifier\n",
        "from skorch.dataset import Dataset\n",
        "from sklearn.model_selection import GridSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qUwxMYif8lrh"
      },
      "outputs": [],
      "source": [
        "## Necessary Parameters\n",
        "\n",
        "batch_size = 512\n",
        "epochs = 12\n",
        "learning_rate = 1e-4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iFxqpF2U6dfP"
      },
      "outputs": [],
      "source": [
        "transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
        "\n",
        "train_dataset = torchvision.datasets.MNIST(\n",
        "    root=\"~/torch_datasets\", train=True, transform=transform, download=True\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset, batch_size=batch_size, shuffle=True\n",
        ")\n",
        "\n",
        "test_dataset = torchvision.datasets.MNIST(\n",
        "    root=\"~/torch_datasets\", train=False, transform=transform, download=True\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset, batch_size=batch_size, shuffle=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uqo9nbbi-cZC"
      },
      "outputs": [],
      "source": [
        "def noise_input(images, NOISE_RATIO):\n",
        "    return images * (1 - NOISE_RATIO) + torch.rand(images.size()) * NOISE_RATIO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vFo-sfaz_sd6"
      },
      "outputs": [],
      "source": [
        "class noisyMNISTDataset(Dataset):\n",
        "    def __init__(self, dataset, NOISE_RATIO):\n",
        "        self.dataset = dataset\n",
        "        self.NOISE_RATIO = NOISE_RATIO\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img, label = self.dataset[index]\n",
        "        noisy_img = noise_input(img, NOISE_RATIO=self.NOISE_RATIO)\n",
        "        noisy_img, img = noisy_img.view(-1), img.view(-1)\n",
        "        return noisy_img, label, img\n",
        "\n",
        "noisy_train_dataset = noisyMNISTDataset(train_dataset, NOISE_RATIO=0.25)\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    noisy_train_dataset, batch_size=batch_size, shuffle=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gdn8XXHO_uci"
      },
      "outputs": [],
      "source": [
        "class denoisingAutoEncoder(nn.Module):\n",
        "  def __init__(self, input_dim):\n",
        "    super(denoisingAutoEncoder, self).__init__()\n",
        "\n",
        "    self.encoder = nn.Sequential(\n",
        "        nn.Linear(input_dim, int(input_dim*0.5)),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.2),\n",
        "        nn.Linear(int(input_dim * 0.5), int(input_dim * 0.25)),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.2)\n",
        "    )\n",
        "\n",
        "    self.decoder = nn.Sequential(\n",
        "        nn.Linear(int(input_dim * 0.25), int(input_dim * 0.5)),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.2),\n",
        "        nn.Linear(int(0.5 * input_dim), int(input_dim)),\n",
        "        nn.ReLU(),\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.encoder(x)\n",
        "    x = self.decoder(x)\n",
        "    return x\n",
        "\n",
        "denoisingModel = denoisingAutoEncoder(input_dim = 784)\n",
        "optimizer = optim.Adam(denoisingModel.parameters(), lr=1e-3)\n",
        "loss_fn = nn.MSELoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h9P2Hv7oBUfP",
        "outputId": "c056e9af-fbe2-4a97-eb62-cdec94543342"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Avg Loss over epoch: 0.0492789\n",
            "Avg Loss over epoch: 0.0289018\n",
            "Avg Loss over epoch: 0.0245378\n",
            "Avg Loss over epoch: 0.0223791\n",
            "Avg Loss over epoch: 0.0210635\n",
            "Avg Loss over epoch: 0.0202625\n",
            "Avg Loss over epoch: 0.0196256\n",
            "Avg Loss over epoch: 0.0191718\n",
            "Avg Loss over epoch: 0.0187599\n",
            "Avg Loss over epoch: 0.0184734\n",
            "Avg Loss over epoch: 0.0182114\n",
            "Avg Loss over epoch: 0.0179731\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "def training_loop(dataloader, model, loss_fn, optimizer):\n",
        "  size = len(dataloader.dataset)\n",
        "  total_loss = 0\n",
        "  num_batches = len(dataloader)\n",
        "\n",
        "  model.train()\n",
        "  for batch, (X, y, Xhat) in enumerate(dataloader):\n",
        "    X, y, Xhat = X, y, Xhat # X : Noisy_Image, y : label, Xhat : Image\n",
        "\n",
        "    X = model(X)\n",
        "    loss = loss_fn(X, Xhat)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    total_loss += loss.item()\n",
        "\n",
        "  avg_loss = total_loss / num_batches\n",
        "  print(f\"Avg Loss over epoch: {avg_loss:>.7f}\")\n",
        "\n",
        "  return avg_loss\n",
        "\n",
        "train_losses = []\n",
        "\n",
        "for t in range(epochs):\n",
        "    train_losses.append(training_loop(train_loader, denoisingModel, loss_fn, optimizer))\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SPCZvu1NCaqa",
        "outputId": "7f485f85-d833-47e9-da3e-dd880c5acf57"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train feature shape: torch.Size([60000, 196])\n",
            "Train label shape: torch.Size([60000])\n"
          ]
        }
      ],
      "source": [
        "# Define the noisy test dataset and DataLoader\n",
        "noisy_test_dataset = noisyMNISTDataset(test_dataset, NOISE_RATIO=0.25)\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    noisy_test_dataset, batch_size=batch_size, shuffle=False\n",
        ")\n",
        "\n",
        "def extract_features(model, dataloader):\n",
        "    model.eval()\n",
        "    features = []\n",
        "    labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y, Xhat in dataloader:\n",
        "            encoded_features = model.encoder(X)\n",
        "            features.append(encoded_features)\n",
        "            labels.append(y)\n",
        "\n",
        "    return torch.cat(features), torch.cat(labels)\n",
        "\n",
        "# Extract features for training and testing datasets\n",
        "train_features, train_labels = extract_features(denoisingModel, train_loader)\n",
        "test_features, test_labels = extract_features(denoisingModel, test_loader)\n",
        "\n",
        "# Print feature shapes\n",
        "print(\"Train feature shape:\", train_features.shape)  # Expected: [60000, latent_dim]\n",
        "print(\"Train label shape:\", train_labels.shape)      # Expected: [60000]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEuOtd5dLEQ_"
      },
      "source": [
        "### Training MLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2pCIdvYHK5IS"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "  def __init__(self, input_dim=196, output_l1_dim=98, dropout_ratio=0.2):\n",
        "    super().__init__()\n",
        "    self.classifier = nn.Sequential(\n",
        "        nn.Linear(input_dim, output_l1_dim),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(dropout_ratio),\n",
        "        nn.Linear(output_l1_dim, 10),\n",
        "        nn.Softmax(dim=1)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.classifier(x)\n",
        "\n",
        "classifierModel = MLP()\n",
        "optimizer = optim.Adam(classifierModel.parameters(), lr=1e-3)\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UGwnCaHfNyoi"
      },
      "outputs": [],
      "source": [
        "def train_mlp(model, dataloader, loss_fn, optimizer):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    for X, y in dataloader:\n",
        "        # Forward pass\n",
        "        preds = model(X)\n",
        "        loss = loss_fn(preds, y)\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Metrics tracking\n",
        "        total_loss += loss.item()\n",
        "        _, predicted_classes = torch.max(preds, dim=1)\n",
        "        correct += (predicted_classes == y).sum().item()\n",
        "        total_samples += y.size(0)\n",
        "\n",
        "    accuracy = correct / total_samples\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    print(f\"Train Loss: {avg_loss:.4f}, Train Accuracy: {accuracy:.4f}\")\n",
        "    return avg_loss, accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "suu4W0NTOODF"
      },
      "outputs": [],
      "source": [
        "def evaluate_mlp(model, dataloader, loss_fn):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            preds = model(X)\n",
        "            loss = loss_fn(preds, y)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            _, predicted_classes = torch.max(preds, dim=1)\n",
        "            correct += (predicted_classes == y).sum().item()\n",
        "            total_samples += y.size(0)\n",
        "\n",
        "    accuracy = correct / total_samples\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    print(f\"Test Loss: {avg_loss:.4f}, Test Accuracy: {accuracy:.4f}\")\n",
        "    return avg_loss, accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xKAvD4nuOSBA"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "\n",
        "# Wrap features and labels into TensorDataset\n",
        "train_dataset = torch.utils.data.TensorDataset(train_features, train_labels)\n",
        "test_dataset = torch.utils.data.TensorDataset(test_features, test_labels)\n",
        "\n",
        "# Create DataLoader\n",
        "train_loader_classification = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader_classification = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V8nB_ZU3OVQG",
        "outputId": "7b4d366c-a321-4069-8987-dd5d6257eeb8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "Train Loss: 1.6698, Train Accuracy: 0.8445\n",
            "Test Loss: 1.5488, Test Accuracy: 0.9276\n",
            "Epoch 2/10\n",
            "Train Loss: 1.5543, Train Accuracy: 0.9192\n",
            "Test Loss: 1.5304, Test Accuracy: 0.9395\n",
            "Epoch 3/10\n",
            "Train Loss: 1.5365, Train Accuracy: 0.9338\n",
            "Test Loss: 1.5198, Test Accuracy: 0.9477\n",
            "Epoch 4/10\n",
            "Train Loss: 1.5261, Train Accuracy: 0.9433\n",
            "Test Loss: 1.5139, Test Accuracy: 0.9515\n",
            "Epoch 5/10\n",
            "Train Loss: 1.5183, Train Accuracy: 0.9495\n",
            "Test Loss: 1.5091, Test Accuracy: 0.9571\n",
            "Epoch 6/10\n",
            "Train Loss: 1.5139, Train Accuracy: 0.9531\n",
            "Test Loss: 1.5035, Test Accuracy: 0.9627\n",
            "Epoch 7/10\n",
            "Train Loss: 1.5092, Train Accuracy: 0.9575\n",
            "Test Loss: 1.5024, Test Accuracy: 0.9625\n",
            "Epoch 8/10\n",
            "Train Loss: 1.5067, Train Accuracy: 0.9597\n",
            "Test Loss: 1.4979, Test Accuracy: 0.9667\n",
            "Epoch 9/10\n",
            "Train Loss: 1.5030, Train Accuracy: 0.9627\n",
            "Test Loss: 1.4979, Test Accuracy: 0.9661\n",
            "Epoch 10/10\n",
            "Train Loss: 1.5008, Train Accuracy: 0.9647\n",
            "Test Loss: 1.4965, Test Accuracy: 0.9681\n"
          ]
        }
      ],
      "source": [
        "epochs = 10\n",
        "train_losses = []\n",
        "train_accuracies = []\n",
        "test_losses = []\n",
        "test_accuracies = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
        "    train_loss, train_acc = train_mlp(classifierModel, train_loader_classification, loss_fn, optimizer)\n",
        "    test_loss, test_acc = evaluate_mlp(classifierModel, test_loader_classification, loss_fn)\n",
        "\n",
        "    # Log metrics\n",
        "    train_losses.append(train_loss)\n",
        "    train_accuracies.append(train_acc)\n",
        "    test_losses.append(test_loss)\n",
        "    test_accuracies.append(test_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SUDWn2zO5dN"
      },
      "source": [
        "## HyperParameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "D2cUWym0O7pW",
        "outputId": "bdee78b4-9a5b-4de7-971d-e4f67649218a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 81 candidates, totalling 243 fits\n",
            "  epoch    train_loss    valid_acc    valid_loss     dur\n",
            "-------  ------------  -----------  ------------  ------\n",
            "      1        \u001b[36m1.6722\u001b[0m       \u001b[32m0.9141\u001b[0m        \u001b[35m1.5603\u001b[0m  2.1313\n",
            "      2        \u001b[36m1.5460\u001b[0m       \u001b[32m0.9298\u001b[0m        \u001b[35m1.5387\u001b[0m  1.9811\n",
            "      3        \u001b[36m1.5295\u001b[0m       \u001b[32m0.9407\u001b[0m        \u001b[35m1.5270\u001b[0m  1.9977\n",
            "      4        \u001b[36m1.5195\u001b[0m       \u001b[32m0.9497\u001b[0m        \u001b[35m1.5182\u001b[0m  1.9564\n",
            "      5        \u001b[36m1.5122\u001b[0m       \u001b[32m0.9558\u001b[0m        \u001b[35m1.5120\u001b[0m  2.0165\n",
            "      6        \u001b[36m1.5073\u001b[0m       \u001b[32m0.9586\u001b[0m        \u001b[35m1.5080\u001b[0m  2.9806\n",
            "      7        \u001b[36m1.5035\u001b[0m       \u001b[32m0.9597\u001b[0m        \u001b[35m1.5061\u001b[0m  2.6102\n",
            "      8        \u001b[36m1.5004\u001b[0m       \u001b[32m0.9611\u001b[0m        \u001b[35m1.5028\u001b[0m  2.0243\n",
            "      9        \u001b[36m1.4974\u001b[0m       \u001b[32m0.9623\u001b[0m        \u001b[35m1.5014\u001b[0m  1.9977\n",
            "     10        \u001b[36m1.4950\u001b[0m       \u001b[32m0.9641\u001b[0m        \u001b[35m1.4993\u001b[0m  2.0529\n",
            "     11        \u001b[36m1.4931\u001b[0m       \u001b[32m0.9655\u001b[0m        \u001b[35m1.4978\u001b[0m  2.9463\n",
            "     12        \u001b[36m1.4915\u001b[0m       \u001b[32m0.9673\u001b[0m        \u001b[35m1.4961\u001b[0m  3.2588\n",
            "     13        \u001b[36m1.4902\u001b[0m       \u001b[32m0.9679\u001b[0m        \u001b[35m1.4956\u001b[0m  2.7135\n",
            "     14        \u001b[36m1.4885\u001b[0m       0.9676        \u001b[35m1.4954\u001b[0m  1.9549\n",
            "     15        \u001b[36m1.4871\u001b[0m       \u001b[32m0.9694\u001b[0m        \u001b[35m1.4942\u001b[0m  1.9559\n",
            "     16        \u001b[36m1.4868\u001b[0m       0.9694        \u001b[35m1.4940\u001b[0m  1.9652\n",
            "     17        \u001b[36m1.4852\u001b[0m       \u001b[32m0.9708\u001b[0m        \u001b[35m1.4919\u001b[0m  1.9558\n",
            "     18        \u001b[36m1.4841\u001b[0m       \u001b[32m0.9722\u001b[0m        \u001b[35m1.4915\u001b[0m  2.2234\n",
            "     19        \u001b[36m1.4833\u001b[0m       0.9713        \u001b[35m1.4909\u001b[0m  2.8988\n",
            "     20        \u001b[36m1.4824\u001b[0m       0.9719        \u001b[35m1.4909\u001b[0m  2.3383\n",
            "     21        \u001b[36m1.4820\u001b[0m       \u001b[32m0.9735\u001b[0m        \u001b[35m1.4889\u001b[0m  2.0243\n",
            "     22        \u001b[36m1.4812\u001b[0m       \u001b[32m0.9742\u001b[0m        \u001b[35m1.4887\u001b[0m  1.9552\n",
            "     23        \u001b[36m1.4802\u001b[0m       0.9733        1.4889  1.9857\n",
            "     24        \u001b[36m1.4798\u001b[0m       \u001b[32m0.9752\u001b[0m        \u001b[35m1.4880\u001b[0m  1.9542\n",
            "     25        \u001b[36m1.4786\u001b[0m       0.9747        \u001b[35m1.4878\u001b[0m  2.5240\n",
            "     26        \u001b[36m1.4780\u001b[0m       0.9748        1.4879  2.7995\n",
            "     27        \u001b[36m1.4778\u001b[0m       0.9748        1.4883  1.9573\n",
            "     28        \u001b[36m1.4775\u001b[0m       \u001b[32m0.9754\u001b[0m        \u001b[35m1.4874\u001b[0m  1.9606\n",
            "     29        \u001b[36m1.4770\u001b[0m       \u001b[32m0.9765\u001b[0m        \u001b[35m1.4863\u001b[0m  1.9746\n",
            "     30        \u001b[36m1.4766\u001b[0m       0.9757        1.4872  2.0064\n",
            "Best Hyperparameters: {'lr': 0.001, 'max_epochs': 30, 'module__dropout_ratio': 0.2, 'module__output_l1_dim': 196}\n",
            "Best Cross-Validation Accuracy: 0.9724\n",
            "Test Accuracy with Best Hyperparameters: 0.9757\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from skorch import NeuralNetClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_dim=196, output_l1_dim=98, dropout_ratio=0.2):\n",
        "        super().__init__()\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(input_dim, output_l1_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_ratio),\n",
        "            nn.Linear(output_l1_dim, 10),\n",
        "            nn.Softmax(dim=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.classifier(x)\n",
        "\n",
        "net = NeuralNetClassifier(\n",
        "    MLP,\n",
        "    module__input_dim=196,\n",
        "    criterion=nn.CrossEntropyLoss,\n",
        "    optimizer=optim.Adam,\n",
        "    max_epochs=10,\n",
        "    lr=0.001,\n",
        "    batch_size=64,\n",
        "    device='cuda' if torch.cuda.is_available() else 'cpu'\n",
        ")\n",
        "\n",
        "param_grid = {\n",
        "    'module__output_l1_dim': [64, 128, 196],\n",
        "    'module__dropout_ratio': [0.2, 0.4, 0.6],\n",
        "    'lr': [1e-3, 1e-4, 5e-4],\n",
        "    'max_epochs': [10, 20, 30]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=net,\n",
        "    param_grid=param_grid,\n",
        "    cv=3,\n",
        "    scoring='accuracy',\n",
        "    verbose=2,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "train_data = torch.utils.data.TensorDataset(train_features, train_labels)\n",
        "test_data = torch.utils.data.TensorDataset(test_features, test_labels)\n",
        "\n",
        "grid_search.fit(train_features.numpy(), train_labels.numpy())\n",
        "\n",
        "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
        "print(\"Best Cross-Validation Accuracy:\", grid_search.best_score_)\n",
        "\n",
        "best_model = grid_search.best_estimator_\n",
        "test_preds = best_model.predict(test_features.numpy())\n",
        "test_accuracy = accuracy_score(test_labels.numpy(), test_preds)\n",
        "print(\"Test Accuracy with Best Hyperparameters:\", test_accuracy)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}